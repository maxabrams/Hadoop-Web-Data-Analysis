# Hadoop-Web-Data-Analysis
A demonstration of Hadoop being used to process web dataBased on assignments by Kenneth Heafield, Michail Basios and Stratis Viglas. The files (on HDFS) that you will need for each part follows (These will need to be generated):Part 1File Number of lines Number of words/data/assignments/ex1/webSmall.txt 95350 6311838/data/assignments/ex1/webLarge.txt 1897987 123588873Part 2Files Dimensions/data/assignments/ex1/matrixSmall.txt (20, 10)/data/assignments/ex1/matrixLarge.txt (4000, 3000)Part3File Number of lines Number of words/data/assignments/ex1/uniSmall.txt 2000 6987/data/assignments/ex1/uniLarge.txt 2000000 6985354All your actual results should be produced using the larger files and for alltasks you should use Hadoop MAPREDUCE and HDFS.Tasks1.1	Processing Web dataTask 1 Take file webLarge.txt, and produce a version which is all lower-case. Forexample, the sentence John loves Mary. would become john loves mary.Call this the lower-case version. It does not matter if the output is a single fileor multiple files. It does not matter if the order of the lines changes; you arenot required to preserve line order.Task 2 Remove duplicated lines from the produced lower-case version. Call this thededuplicated version. Your approach should be exact (make no mistakes). Theoutput will be used for some of the following tasks. Task 3 Look at the Unix command wc. Implement an exact version of it (which onlycounts words and lines) for Hadoop, using MAPREDUCE and any appropriateUnix commands. You should assume the input data and results are stored inHDFS. The output results from HDFS should be merged into a single file (onUnix) with the total number of words and lines. For example, the followingfile example.txt:bob had a little lamb and a small catalice had one tigermary had some small dogs and a rabbitshould have the following result (21 is the total number of words and 3 is thetotal number of lines):21 3Run your implementation on the deduplicated data from task 2. Task 4 On the deduplicated version, find all two-word sequences and their counts.For example, the two sentences:mary had a lambmary had a tigerhave the following two-word sequences and counts:Sequence Countmary had 2had a 2a lamb 1a tiger 1Task 5 Create a version of the two-word counting program that uses a combiner. Is itfaster? Task 6 What are the top twenty most frequent two-word sequences? 1.2 Matrix TransposeTask 7 Create a program that uses MAPREDUCE and takes as an input a large filecontaining a matrix /data/assignments/ex1/largeMatrix.txt and returnsan output file that has it transposed. The final result can be either a singlefile or multiple files (in which case, when concatenated, they should give thesame result as a single file).For example, the input matrix:1 2 3 45 6 7 89 10 11 12should be transposed to:1 5 92 6 103 7 114 8 121.3 Relational Join using MAPREDUCEIn this task you will perform a join operation in Hadoop. Let us assume that wehave the relations student(studentId, name) and marks(courseId, studentId,mark) as shown below:studentsstudentId name1 George2 AnnamarkscourseId studentId markEXC 1 70EXC 2 65TTS 1 70ADBS 1 80and need to join them on the studentId field. Traditionally, this is an easy taskwhen we deal with relational databases and can be performed by using therelational join operator. However, the way this join operation is performeddrastically changes, when we assume our input is into a single file that storesinformation from both relations.Assume the format of such a single input file storing data from two relationsis as follows:student 1 Georgemark EXC 1 70student 2 Annamark ADBS 1 80mark EXC 2 65mark TTS 1 80The first column is a tag that shows from which relation the data comesfrom. Depending on this tag, we can assign meaning to the other columns.When the tag used is mark, we know that the second column refers to thecourseId, the third to the studentId and the fourth refers to the grade thestudent took in this specific course. On the other hand, if the tag is student,we know that there are only two other columns, one with the studentId andone with the student name.Task 8 Use the uniLarge.txt file perform a join operation on the studentId key andproduce an output that will have the grades of each student as follows:name --> (course1, mark1) (course2, mark2) (course3, mark3) . . .For example, for the previous input file your algorithm should return:George --> (ADBS, 80) (EXC, 70) (TTS, 80)Anna --> (EXC, 65) Task 9 What is the fictional name of the student (or students in case of equality) withthe lowest average when the number of lessons examined is greater than four?